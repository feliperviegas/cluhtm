{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tf_idf_repr(topics, cw_words, tf_idf_t):\n",
    "    cw_frequency = {}\n",
    "    cw_docs = {}\n",
    "    for iter_topic in topics:\n",
    "        for word in iter_topic:\n",
    "            word_index = np.where(cw_words == word)[0]\n",
    "            cw_frequency[word] = float(tf_idf_t[word_index].data.shape[0])\n",
    "            cw_docs[word] = set(tf_idf_t[word_index].nonzero()[1])\n",
    "\n",
    "    n_docs = float(tf_idf_t.data.shape[0])\n",
    "\n",
    "    return cw_frequency, cw_docs, n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(topics, word_frequency, term_docs, n_docs, n_top_words):\n",
    "    pmi = []\n",
    "    npmi = []\n",
    "\n",
    "    n_top_words = float(n_top_words)\n",
    "\n",
    "    for t in range(len(topics)):\n",
    "        top_w = topics[t]\n",
    "        # top_w = topico.split(' ')\n",
    "\n",
    "        pmi_t = 0.0\n",
    "        npmi_t = 0.0\n",
    "\n",
    "        for j in range(1, len(top_w)):\n",
    "            for i in range(0, j):\n",
    "                ti = top_w[i]\n",
    "                tj = top_w[j]\n",
    "\n",
    "                c_i = word_frequency[ti]\n",
    "                c_j = word_frequency[tj]\n",
    "                c_i_and_j = len(term_docs[ti].intersection(term_docs[tj]))\n",
    "\n",
    "                pmi_t += np.log(((c_i_and_j + 1.0) / float(n_docs)) /\n",
    "                                ((c_i * c_j) / float(n_docs) ** 2))\n",
    "\n",
    "                npmi_t += -1.0 * np.log((c_i_and_j + 0.01) / float(n_docs))\n",
    "\n",
    "        peso = 1.0 / (n_top_words * (n_top_words - 1.0))\n",
    "\n",
    "        pmi.append(peso * pmi_t)\n",
    "        npmi.append(pmi_t / npmi_t)\n",
    "\n",
    "    return pmi, npmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence(topics, word_frequency, term_docs):\n",
    "    coherence = []\n",
    "\n",
    "    for t in range(len(topics)):\n",
    "        top_w = topics[t]\n",
    "\n",
    "        coherence_t = 0.0\n",
    "        for i in range(1, len(top_w)):\n",
    "            for j in range(0, i):\n",
    "                cont_wi = word_frequency[top_w[j]]\n",
    "                cont_wi_wj = float(\n",
    "                    len(term_docs[top_w[j]].intersection(term_docs[top_w[i]])))\n",
    "                coherence_t += np.log((cont_wi_wj + 1.0) / cont_wi)\n",
    "\n",
    "        coherence.append(coherence_t)\n",
    "\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V-L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def w2v_metric(topics, word_embedding_path, distance_type=\"cos_dist\", top_words=10, embedding_type=False):\n",
    "        word_vectors = KeyedVectors.load_word2vec_format(f\"{word_embedding_path}\", binary=embedding_type)\n",
    "#         model = word_vectors.wv\n",
    "        values = []\n",
    "\n",
    "        for words in topics:\n",
    "#             words = topic.split(' ')\n",
    "            value = calc_dist_2(words, word_vectors, distance_type, top_words)\n",
    "            values.append(value)\n",
    "\n",
    "        return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as sci_dist\n",
    "\n",
    "def calc_dist_2(words, w2v_model, distance_type, t):\n",
    "    l1_dist = 0\n",
    "    l2_dist = 0\n",
    "    cos_dist = 0\n",
    "    coord_dist = 0\n",
    "    t = float(t)\n",
    "\n",
    "    for word_id1 in range(len(words)):\n",
    "        for word_id2 in range(word_id1 + 1, len(words)):\n",
    "            # Calcular L1 w2v metric\n",
    "            l1_dist += (sci_dist.euclidean(\n",
    "                w2v_model[words[word_id1]], w2v_model[words[word_id2]]))\n",
    "\n",
    "            # Calcular L2 w2v metric\n",
    "            l2_dist += (sci_dist.sqeuclidean(\n",
    "                w2v_model[words[word_id1]], w2v_model[words[word_id2]]))\n",
    "\n",
    "            # Calcular cos w2v metric\n",
    "            cos_dist += (sci_dist.cosine(\n",
    "                w2v_model[words[word_id1]], w2v_model[words[word_id2]]))\n",
    "\n",
    "            # Calcular coordinate w2v metric\n",
    "            coord_dist += (sci_dist.sqeuclidean(\n",
    "                w2v_model[words[word_id1]], w2v_model[words[word_id2]]))\n",
    "\n",
    "    if distance_type == 'l1_dist':\n",
    "        return l1_dist / (t * (t - 1.0))\n",
    "    elif distance_type == 'l2_dist':\n",
    "        return l2_dist / (t * (t - 1.0))\n",
    "    elif distance_type == 'cos_dist':\n",
    "        return cos_dist / (t * (t - 1.0))\n",
    "    elif distance_type == 'coord_dist':\n",
    "        return coord_dist / (t * (t - 1.0))\n",
    "\n",
    "    return .0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_path):\n",
    "    arq = open(input_path, 'r', encoding=\"utf-8\")\n",
    "    doc = arq.readlines()\n",
    "    arq.close()\n",
    "    documents = list(map(str.rstrip, doc))\n",
    "    n_documents = len(documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    vectorizer = TfidfVectorizer(encoding='utf-8', \n",
    "                                 analyzer='word', \n",
    "                                 max_df=1.0, \n",
    "                                 min_df=1,\n",
    "                                 norm='l2', \n",
    "                                 use_idf=True, \n",
    "                                 smooth_idf=False, \n",
    "                                 sublinear_tf=True)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18828, 40088), 40088)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"20News\"\n",
    "source_dataset = \"Baselines/textual_folds\"\n",
    "dataset_input_file = f\"{source_dataset}/{dataset}/{dataset}Pre.txt\"\n",
    "\n",
    "data = read_input(input_path=dataset_input_file)\n",
    "X, feature_names = tfidf(data=data)\n",
    "X.shape, len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline=\"bertopic\"\n",
    "top_words = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [], 1: [], 2: []}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if baseline == 'bertopic':\n",
    "    source = \"Baselines/BertTopicsResultsHierarchical\"\n",
    "    hierarchical_file = f\"{source}/topic_words_{top_words}_{dataset}.txt\"  \n",
    "elif baseline == 'hartm':\n",
    "    source = \"Baselines/hArtmTopicResults\"\n",
    "    hierarchical_file = f\"{source}/topic_words_{top_words}_{dataset}Pre\"\n",
    "elif baseline == \"hpam\":\n",
    "    source = \"../HPAMResults\"\n",
    "    hierarchical_file = f\"{source}/{dataset}.txt\"  \n",
    "elif baseline == \"hlda\":\n",
    "    source = \"\"\n",
    "    hierarchical_file = f\"{source}/{dataset}.txt\"\n",
    "    \n",
    "topics = {\n",
    "    0: list(),\n",
    "    1: list(),\n",
    "    2: list()\n",
    "}\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "if baseline == \"bertopic\" or baseline == \"hartm\":\n",
    "    with open(hierarchical_file) as hierachical_input:\n",
    "        for topic in hierachical_input:\n",
    "            if topic.startswith(\"\\t\\t\"):\n",
    "                topics[2].append(topic.replace(\"\\t\\t\", \"\").strip().split(\" \"))\n",
    "            elif topic.startswith(\"\\t\"):\n",
    "                topics[1].append(topic.replace(\"\\t\", \"\").strip().split(\" \"))\n",
    "            else:\n",
    "                topics[0].append(topic.strip().split(\" \"))\n",
    "        \n",
    "        hierachical_input.close()      \n",
    "elif baseline == \"hpam\":\n",
    "    with open(hierarchical_file) as hierachical_input:\n",
    "        for topic in hierachical_input:\n",
    "            if topic.startswith(\"Super-topic\"):\n",
    "                topics[1].append(topic.replace(\"\\t\", \" \").strip().split(\" \")[4:])\n",
    "            elif topic.startswith(\"Root:\"):\n",
    "                topics[0].append(topic.replace(\"]\", \" \").strip().split(\" \")[2:])\n",
    "            elif re.match(\"[0-9]+:\", topic):\n",
    "                topics[2].append(topic.replace(\"\\t\", \" \").strip().split(\" \")[2:])\n",
    "        \n",
    "        hierachical_input.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_freq, features_docs, n_docs = count_tf_idf_repr(topics[0],\n",
    "                                                         np.asarray(feature_names),\n",
    "                                                         csr_matrix(X).transpose())\n",
    "\n",
    "pmi_0, npmi_0 = pmi(topics=topics[0],\n",
    "                    word_frequency=features_freq,\n",
    "                    term_docs=features_docs,\n",
    "                    n_docs=n_docs,\n",
    "                    n_top_words=top_words)\n",
    "\n",
    "coherence_0 = coherence(\n",
    "                      topics=topics[0],\n",
    "                      word_frequency=features_freq,\n",
    "                      term_docs=features_docs\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_score_0 = w2v_metric(\n",
    "#                         topics=topics[0], \n",
    "#                         word_embedding_path=\"/home/felipeviegas/Codes_phd/cluhtm/wiki-news-300d-1M.vec\"\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_freq, features_docs, n_docs = count_tf_idf_repr(topics[1],\n",
    "                                                         np.asarray(feature_names),\n",
    "                                                         csr_matrix(X).transpose())\n",
    "\n",
    "pmi_1, npmi_1 = pmi(topics=topics[1],\n",
    "                    word_frequency=features_freq,\n",
    "                    term_docs=features_docs,\n",
    "                    n_docs=n_docs,\n",
    "                    n_top_words=top_words)\n",
    "\n",
    "coherence_1 = coherence(\n",
    "                      topics=topics[1],\n",
    "                      word_frequency=features_freq,\n",
    "                      term_docs=features_docs\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_score_1 = w2v_metric(\n",
    "#                         topics=topics[1], \n",
    "#                         word_embedding_path=\"/home/felipeviegas/Codes_phd/cluhtm/wiki-news-300d-1M.vec\"\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluwords_freq, cluwords_docs, n_docs = count_tf_idf_repr(topics[2],\n",
    "                                                         np.asarray(feature_names),\n",
    "                                                         csr_matrix(X).transpose())\n",
    "\n",
    "pmi_2, npmi_2 = pmi(topics=topics[2],\n",
    "                    word_frequency=features_freq,\n",
    "                    term_docs=features_docs,\n",
    "                    n_docs=n_docs,\n",
    "                    n_top_words=top_words)\n",
    "\n",
    "coherence_2 = coherence(\n",
    "                      topics=topics[2],\n",
    "                      word_frequency=features_freq,\n",
    "                      term_docs=features_docs\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_score_2 = w2v_metric(\n",
    "#                         topics=topics[2], \n",
    "#                         word_embedding_path=\"/home/felipeviegas/Codes_phd/cluhtm/wiki-news-300d-1M.vec\"\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMPI\n",
      "Depth_0 0.6294408935051241 0.06265701880019196\n",
      "Depth_1 nan nan\n",
      "Depth_2 nan nan\n",
      "Overall 0.6294408935051241 0.06265701880019196\n"
     ]
    }
   ],
   "source": [
    "print(\"NPMPI\")\n",
    "print(f\"Depth_0 {np.mean(npmi_0)} {np.std(npmi_0, ddof=1)}\")\n",
    "print(f\"Depth_1 {np.mean(npmi_1)} {np.std(npmi_1, ddof=1)}\")\n",
    "print(f\"Depth_2 {np.mean(npmi_2)} {np.std(npmi_2, ddof=1)}\")\n",
    "sum_eval = []\n",
    "sum_eval += npmi_0 if npmi_0 else []\n",
    "sum_eval += npmi_1 if npmi_1 else []\n",
    "sum_eval += npmi_2 if npmi_2 else []\n",
    "\n",
    "print(f\"Overall {np.mean(sum_eval)} {np.std(sum_eval, ddof=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence\n",
      "Depth_0 -90.43419921585482 19.650141268475164\n",
      "Depth_1 nan nan\n",
      "Depth_2 nan nan\n",
      "Overall -90.43419921585482 19.650141268475164\n"
     ]
    }
   ],
   "source": [
    "print(\"Coherence\")\n",
    "print(f\"Depth_0 {np.mean(coherence_0)} {np.std(coherence_0, ddof=1)}\")\n",
    "print(f\"Depth_1 {np.mean(coherence_1)} {np.std(coherence_1, ddof=1)}\")\n",
    "print(f\"Depth_2 {np.mean(coherence_2)} {np.std(coherence_2, ddof=1)}\")\n",
    "\n",
    "sum_eval = []\n",
    "sum_eval += coherence_0 if coherence_0 else []\n",
    "sum_eval += coherence_1 if coherence_1 else []\n",
    "sum_eval += coherence_2 if coherence_2 else []\n",
    "\n",
    "print(f\"Overall {np.mean(sum_eval)} {np.std(sum_eval, ddof=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"W2V-L1\")\n",
    "# print(f\"Depth_0 {np.mean(w2v_score_0)} {np.std(w2v_score_0, ddof=1)}\")\n",
    "# print(f\"Depth_1 {np.mean(w2v_score_1)} {np.std(w2v_score_1, ddof=1)}\")\n",
    "# print(f\"Depth_2 {np.mean(w2v_score_2)} {np.std(w2v_score_2, ddof=1)}\")\n",
    "# print(f\"Overall {np.mean(w2v_score_0 + w2v_score_1 + w2v_score_2)} {np.std(w2v_score_0 + w2v_score_1 + w2v_score_2, ddof=1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluwords_emb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd5b5d06706bb19d070e38d4ebad3eab8249ed08b8bc628803b1920b913342d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

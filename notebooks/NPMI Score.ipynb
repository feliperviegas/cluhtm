{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TFIDF Represantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_path):\n",
    "    arq = open(input_path, 'r', encoding=\"utf-8\")\n",
    "    doc = arq.readlines()\n",
    "    arq.close()\n",
    "    documents = list(map(str.rstrip, doc))\n",
    "    n_documents = len(documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    vectorizer = TfidfVectorizer(encoding='utf-8', \n",
    "                                 analyzer='word', \n",
    "                                 max_df=1.0, \n",
    "                                 min_df=1,\n",
    "                                 norm='l2', \n",
    "                                 use_idf=True, \n",
    "                                 smooth_idf=False, \n",
    "                                 sublinear_tf=True)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfidf(dataset_input_file):\n",
    "    data = read_input(input_path=dataset_input_file)\n",
    "    X, feature_names = tfidf(data=data)\n",
    "    \n",
    "    return X, np.asarray(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPMI Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tf_idf_repr(topics, cw_words, tf_idf_t):\n",
    "    cw_frequency = {}\n",
    "    cw_docs = {}\n",
    "    for iter_topic in topics:\n",
    "        for word in iter_topic:\n",
    "            word_index = np.where(cw_words == word)[0]\n",
    "            cw_frequency[word] = float(tf_idf_t[word_index].data.shape[0])\n",
    "            cw_docs[word] = set(tf_idf_t[word_index].nonzero()[1])\n",
    "\n",
    "    n_docs = float(tf_idf_t.data.shape[0])\n",
    "\n",
    "    return cw_frequency, cw_docs, n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(topics, word_frequency, term_docs, n_docs, n_top_words):\n",
    "    pmi = []\n",
    "    npmi = []\n",
    "    n_exceptions = 0\n",
    "\n",
    "    n_top_words = float(n_top_words)\n",
    "\n",
    "    for t in range(len(topics)):\n",
    "        top_w = topics[t]\n",
    "        # top_w = topico.split(' ')\n",
    "\n",
    "        pmi_t = 0.0\n",
    "        npmi_t = 0.0\n",
    "\n",
    "        for j in range(1, len(top_w)):\n",
    "            for i in range(0, j):\n",
    "                ti = top_w[i]\n",
    "                tj = top_w[j]\n",
    "\n",
    "                c_i = word_frequency[ti]\n",
    "                c_j = word_frequency[tj]\n",
    "                c_i_and_j = len(term_docs[ti].intersection(term_docs[tj]))\n",
    "                \n",
    "                try:\n",
    "                    pmi_t += np.log(((c_i_and_j + 1.0) / float(n_docs)) /\n",
    "                                    ((c_i * c_j) / float(n_docs) ** 2))\n",
    "                except ZeroDivisionError:\n",
    "                    n_exceptions += 1\n",
    "                    pmi_t += .0\n",
    "\n",
    "                npmi_t += -1.0 * np.log((c_i_and_j + 0.01) / float(n_docs))\n",
    "\n",
    "        peso = 1.0 / (n_top_words * (n_top_words - 1.0))\n",
    "\n",
    "        pmi.append(peso * pmi_t)\n",
    "        npmi.append(pmi_t / npmi_t)\n",
    "\n",
    "    return pmi, npmi, n_exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hierarchical(option):\n",
    "    topics = {\n",
    "        0: list(),\n",
    "        1: list(),\n",
    "        2: list()\n",
    "    }\n",
    "    if option == 'hpam':\n",
    "        with open(hierarchical_file) as hierachical_input:\n",
    "            for topic in hierachical_input:\n",
    "                if topic.startswith(\"Super-topic\"):\n",
    "                    topics[1].append(topic.replace(\"\\t\", \" \").strip().split(\" \")[4:])\n",
    "                elif topic.startswith(\"Root:\"):\n",
    "                    topics[0].append(topic.replace(\"]\", \" \").strip().split(\" \")[2:])\n",
    "                elif re.match(\"[0-9]+:\", topic):\n",
    "                    topics[2].append(topic.replace(\"\\t\", \" \").strip().split(\" \")[2:])\n",
    "\n",
    "            hierachical_input.close()\n",
    "\n",
    "    else:\n",
    "        with open(hierarchical_file) as hierachical_input:\n",
    "            for topic in hierachical_input:\n",
    "                if topic.startswith(\"\\t\\t\"):\n",
    "                    topics[2].append(topic.replace(\"\\t\\t\", \"\").strip().split(\" \"))\n",
    "                elif topic.startswith(\"\\t\"):\n",
    "                    topics[1].append(topic.replace(\"\\t\", \"\").strip().split(\" \"))\n",
    "                else:\n",
    "                    topics[0].append(topic.strip().split(\" \"))\n",
    "\n",
    "            hierachical_input.close()\n",
    "        \n",
    "    \n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['wpp','ang','drop','ever','face','info','pinter','trip','tweets','uber','acm','20News']\n",
    "method = 'hpam'\n",
    "\n",
    "# 'tfidf' or 'cw'\n",
    "base_npmi_score = 'tfidf'\n",
    "hierarchical = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npz(npz_input_file):\n",
    "    loaded  = np.load(npz_input_file)\n",
    "    cluwords_repr = loaded['tfidf']\n",
    "    cluwords_vocab = loaded['feature_names']\n",
    "    \n",
    "    return cluwords_repr, cluwords_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wpp 0.3039133354432548 0.025572339326475266 0.5259673131595624 0.0656032260834207 0.3838258845871712 0.043449538335606504 0.39591148581762164 0.06183071876434732\n",
      "ang 0.35387565537057786 0.013446198654510135 0.5538370418113486 0.05116199635023758 0.4053416471694636 0.05048801524652644 0.41825595306694074 0.06615552691574668\n",
      "drop 0.39702958912934716 0.006622662198203445 0.47450989946577243 0.04954394881850429 0.3773078380728068 0.0322079526117803 0.3862424539735834 0.04392429301723971\n",
      "ever 0.37692863547313904 0.005476569545040999 0.39603548367315966 0.032748926039161734 0.3630324715299916 0.0264015047621506 0.3661309065333685 0.02853302656007675\n",
      "face 0.3547297135295019 0.008090634614931678 0.33093371581143355 0.040768944163583724 0.28869201717522075 0.03762620721132944 0.29309250981230545 0.04006120177501992\n",
      "info 0.6520694296811937 0.004294581820931727 0.4958486403284576 0.030765664376022933 0.5300033349560299 0.03783022501778958 0.5280260299871059 0.0401169984840387\n",
      "pinter 0.35068861090650677 0.0219199935650672 0.4849320261212533 0.05764888020387763 0.36661820127081884 0.039522115346011216 0.37713359458739576 0.05349055589977411\n",
      "trip 0.3450597682901011 0.005369218841634811 0.46601542916737054 0.06945040993132623 0.37027263115409204 0.036533742216589137 0.378670965543901 0.048933300921710346\n",
      "tweets 0.2875669582709876 0.02170473287068965 0.30931452419320105 0.03082593491754004 0.2621284888611874 0.03295789306615631 0.266608658435331 0.035405090009267864\n",
      "uber 0.3166708501337347 0.002680417261191159 0.3562612198159617 0.03421132791961646 0.30013123571713524 0.02981502677154984 0.3053369965766385 0.03412693879412462\n",
      "acm 0.5303346402024416 0.005074992122195488 0.4867692177821475 0.03025638114017045 0.4545191991897198 0.06510197055519415 0.4581076282612243 0.06350134703201356\n",
      "20News 0.6120337972426569 0.0025499375984471295 0.608231725614685 0.04922503274237634 0.5572091661909818 0.03870191899455304 0.5622997087611504 0.04244206465787129\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    total_errors = 0\n",
    "    npmi_0_score = list()\n",
    "    npmi_1_score = list()\n",
    "    npmi_2_score = list()\n",
    "    npmi_all_score = list()\n",
    "#     top_sets = [5, 10, 20]\n",
    "    top_sets = [20]\n",
    "    \n",
    "    if base_npmi_score == 'cw':\n",
    "        cw_source = '../fasttext_wiki_bert_max'\n",
    "        npz_input_file = f\"{cw_source}/results/{dataset}/cluwords_representation_{dataset}.npz\"\n",
    "        cluwords_repr, vocab = read_npz(npz_input_file)\n",
    "    else: # 'tfidf'\n",
    "        source_dataset = \"../textual_folds\"\n",
    "        dataset_input_file = f\"{source_dataset}/{dataset}Pre.txt\"\n",
    "        cluwords_repr, vocab = read_tfidf(dataset_input_file)\n",
    "\n",
    "    for top_words in top_sets:\n",
    "        npmi_all = list()\n",
    "        if method == 'cw':\n",
    "            source = \"../fasttext_wiki_bert_max\"\n",
    "            hierarchical_file = f\"{source}/results/{dataset}/hierarchical_struture.txt\"\n",
    "        elif method == 'bertopic':\n",
    "            source = \"../BertTopicResults\"\n",
    "            hierarchical_file = f\"{source}/topic_words_{top_words}_{dataset}Pre\"\n",
    "        elif method == 'hpam':\n",
    "            source = \"../HPAMResults\"\n",
    "            hierarchical_file = f\"{source}/{dataset}.txt\"\n",
    "                \n",
    "\n",
    "        topics = read_hierarchical(method)\n",
    "\n",
    "        words_freq, words_docs, n_docs = count_tf_idf_repr(topics[0],\n",
    "                                                                 vocab,\n",
    "                                                                 csr_matrix(cluwords_repr).transpose())\n",
    "\n",
    "        pmi_0, npmi_0, errors = pmi(topics=topics[0],\n",
    "                                    word_frequency=words_freq,\n",
    "                                    term_docs=words_docs,\n",
    "                                    n_docs=n_docs,\n",
    "                                    n_top_words=top_words)\n",
    "        \n",
    "        if hierarchical:\n",
    "            words_freq, words_docs, n_docs = count_tf_idf_repr(topics[1],\n",
    "                                                                 vocab,\n",
    "                                                                 csr_matrix(cluwords_repr).transpose())\n",
    "            pmi_1, npmi_1, errors = pmi(topics=topics[1],\n",
    "                                    word_frequency=words_freq,\n",
    "                                    term_docs=words_docs,\n",
    "                                    n_docs=n_docs,\n",
    "                                    n_top_words=top_words)\n",
    "            \n",
    "            words_freq, words_docs, n_docs = count_tf_idf_repr(topics[2],\n",
    "                                                                 vocab,\n",
    "                                                                 csr_matrix(cluwords_repr).transpose())\n",
    "            pmi_2, npmi_2, errors = pmi(topics=topics[2],\n",
    "                                    word_frequency=words_freq,\n",
    "                                    term_docs=words_docs,\n",
    "                                    n_docs=n_docs,\n",
    "                                    n_top_words=top_words)\n",
    "            \n",
    "        \n",
    "        total_errors += errors\n",
    "\n",
    "        npmi_0_score.append(np.mean(npmi_0)) \n",
    "        npmi_0_score.append(np.std(npmi_0, ddof=1))\n",
    "        npmi_all += npmi_0\n",
    "        \n",
    "        if hierarchical:\n",
    "            npmi_1_score.append(np.mean(npmi_1)) \n",
    "            npmi_1_score.append(np.std(npmi_1, ddof=1))\n",
    "            npmi_all += npmi_1\n",
    "            \n",
    "            npmi_2_score.append(np.mean(npmi_2)) \n",
    "            npmi_2_score.append(np.std(npmi_2, ddof=1))\n",
    "            npmi_all += npmi_2\n",
    "            \n",
    "            npmi_all_score.append(np.mean(npmi_all)) \n",
    "            npmi_all_score.append(np.std(npmi_all, ddof=1))\n",
    "    \n",
    "#     print(f\"{dataset} {' '.join([str(score) for score in npmi_0_score])} -- {total_errors}\")\n",
    "    if hierarchical:\n",
    "        print(f\"{dataset} {' '.join([str(score) for score in npmi_0_score])} {' '.join([str(score) for score in npmi_1_score])} {' '.join([str(score) for score in npmi_2_score])} {' '.join([str(score) for score in npmi_all_score])}\")\n",
    "    else:\n",
    "        print(f\"{dataset} {' '.join([str(score) for score in npmi_0_score])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TFIDF Represantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_path):\n",
    "    arq = open(input_path, 'r', encoding=\"utf-8\")\n",
    "    doc = arq.readlines()\n",
    "    arq.close()\n",
    "    documents = list(map(str.rstrip, doc))\n",
    "    n_documents = len(documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    vectorizer = TfidfVectorizer(encoding='utf-8', \n",
    "                                 analyzer='word', \n",
    "                                 max_df=1.0, \n",
    "                                 min_df=1,\n",
    "                                 norm='l2', \n",
    "                                 use_idf=True, \n",
    "                                 smooth_idf=False, \n",
    "                                 sublinear_tf=True)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfidf(dataset_input_file):\n",
    "    data = read_input(input_path=dataset_input_file)\n",
    "    X, feature_names = tfidf(data=data)\n",
    "    \n",
    "    return X, np.asarray(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPMI Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tf_idf_repr(topics, cw_words, tf_idf_t):\n",
    "    cw_frequency = {}\n",
    "    cw_docs = {}\n",
    "    for iter_topic in topics:\n",
    "        for word in iter_topic:\n",
    "            word_index = np.where(cw_words == word)[0]\n",
    "            cw_frequency[word] = float(tf_idf_t[word_index].data.shape[0])\n",
    "            cw_docs[word] = set(tf_idf_t[word_index].nonzero()[1])\n",
    "\n",
    "    n_docs = float(tf_idf_t.data.shape[0])\n",
    "\n",
    "    return cw_frequency, cw_docs, n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(topics, word_frequency, term_docs, n_docs, n_top_words):\n",
    "    pmi = []\n",
    "    npmi = []\n",
    "    n_exceptions = 0\n",
    "\n",
    "    n_top_words = float(n_top_words)\n",
    "\n",
    "    for t in range(len(topics)):\n",
    "        top_w = topics[t]\n",
    "        # top_w = topico.split(' ')\n",
    "\n",
    "        pmi_t = 0.0\n",
    "        npmi_t = 0.0\n",
    "\n",
    "        for j in range(1, len(top_w)):\n",
    "            for i in range(0, j):\n",
    "                ti = top_w[i]\n",
    "                tj = top_w[j]\n",
    "\n",
    "                c_i = word_frequency[ti]\n",
    "                c_j = word_frequency[tj]\n",
    "                c_i_and_j = len(term_docs[ti].intersection(term_docs[tj]))\n",
    "                \n",
    "                try:\n",
    "                    pmi_t += np.log(((c_i_and_j + 1.0) / float(n_docs)) /\n",
    "                                    ((c_i * c_j) / float(n_docs) ** 2))\n",
    "                except ZeroDivisionError:\n",
    "                    n_exceptions += 1\n",
    "                    pmi_t += .0\n",
    "\n",
    "                npmi_t += -1.0 * np.log((c_i_and_j + 0.01) / float(n_docs))\n",
    "\n",
    "        peso = 1.0 / (n_top_words * (n_top_words - 1.0))\n",
    "\n",
    "        pmi.append(peso * pmi_t)\n",
    "        npmi.append(pmi_t / npmi_t)\n",
    "\n",
    "    return pmi, npmi, n_exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hierarchical(option):\n",
    "    topics = {\n",
    "        0: list(),\n",
    "        1: list(),\n",
    "        2: list()\n",
    "    }\n",
    "    if option == 'hpam':\n",
    "        with open(hierarchical_file) as hierachical_input:\n",
    "            for topic in hierachical_input:\n",
    "                if topic.startswith(\"Super-topic\"):\n",
    "                    topics[1].append(topic.replace(\"\\t\", \" \").strip().split(\" \")[4:])\n",
    "                elif topic.startswith(\"Root:\"):\n",
    "                    topics[0].append(topic.replace(\"]\", \" \").strip().split(\" \")[2:])\n",
    "                elif re.match(\"[0-9]+:\", topic):\n",
    "                    topics[2].append(topic.replace(\"\\t\", \" \").strip().split(\" \")[2:])\n",
    "\n",
    "            hierachical_input.close()\n",
    "\n",
    "    else:\n",
    "        with open(hierarchical_file) as hierachical_input:\n",
    "            for topic in hierachical_input:\n",
    "                if topic.startswith(\"\\t\\t\"):\n",
    "                    topics[2].append(topic.replace(\"\\t\\t\", \"\").strip().split(\" \"))\n",
    "                elif topic.startswith(\"\\t\"):\n",
    "                    topics[1].append(topic.replace(\"\\t\", \"\").strip().split(\" \"))\n",
    "                else:\n",
    "                    topics[0].append(topic.strip().split(\" \"))\n",
    "\n",
    "            hierachical_input.close()\n",
    "        \n",
    "    \n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['wpp','trip']\n",
    "# datasets = ['wpp','ang','drop','ever','face','info','pinter','trip','tweets','uber','acm','20News']\n",
    "# method = 'hpam'\n",
    "method = 'cw'\n",
    "\n",
    "# 'tfidf' or 'cw'\n",
    "base_npmi_score = 'tfidf'\n",
    "hierarchical = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npz(npz_input_file):\n",
    "    loaded  = np.load(npz_input_file)\n",
    "    cluwords_repr = loaded['tfidf']\n",
    "    cluwords_vocab = loaded['feature_names']\n",
    "    \n",
    "    return cluwords_repr, cluwords_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wpp 0.609017906509522 0.04734657055278081 0.617191127887048 0.030916811437335972 0.6128392591761436 0.03416570368103874 0.6134171457431559 0.033932483399341865\n",
      "trip 0.6426477948202447 0.028450877555143757 0.6305205805479069 0.037094295057654894 0.6284501435395424 0.032878523941693695 0.6290999501777472 0.03349919729997857\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    total_errors = 0\n",
    "    npmi_0_score = list()\n",
    "    npmi_1_score = list()\n",
    "    npmi_2_score = list()\n",
    "    npmi_all_score = list()\n",
    "#     top_sets = [5, 10, 20]\n",
    "    top_sets = [10]\n",
    "    \n",
    "    if base_npmi_score == 'cw':\n",
    "        # cw_source = '../fasttext_wiki_bert_max'\n",
    "        cw_source = '../fasttext_wiki'\n",
    "        npz_input_file = f\"{cw_source}/results/{dataset}_seed-45/cluwords_representation_{dataset}.npz\"\n",
    "        # npz_input_file = f\"{cw_source}/results/{dataset}/cluwords_representation_{dataset}.npz\"\n",
    "        # cluwords_repr, vocab = read_npz(npz_input_file)\n",
    "        \n",
    "    else: # 'tfidf'\n",
    "        source_dataset = \"../textual_folds\"\n",
    "        dataset_input_file = f\"{source_dataset}/{dataset}Pre.txt\"\n",
    "        cluwords_repr, vocab = read_tfidf(dataset_input_file)\n",
    "\n",
    "    for top_words in top_sets:\n",
    "        npmi_all = list()\n",
    "        if method == 'cw':\n",
    "            source = \"../fasttext_wiki\"\n",
    "            # source = \"../fasttext_wiki_bert_max\"\n",
    "            # hierarchical_file = f\"{source}/results/{dataset}/hierarchical_struture.txt\"\n",
    "            hierarchical_file = f\"{source}/results/{dataset}_seed-45/hierarchical_struture.txt\"\n",
    "        elif method == 'bertopic':\n",
    "            source = \"../BertTopicResults\"\n",
    "            hierarchical_file = f\"{source}/topic_words_{top_words}_{dataset}Pre\"\n",
    "        elif method == 'hpam':\n",
    "            source = \"../HPAMResults\"\n",
    "            hierarchical_file = f\"{source}/{dataset}.txt\"\n",
    "                \n",
    "\n",
    "        topics = read_hierarchical(method)\n",
    "\n",
    "        words_freq, words_docs, n_docs = count_tf_idf_repr(topics[0],\n",
    "                                                                 vocab,\n",
    "                                                                 csr_matrix(cluwords_repr).transpose())\n",
    "\n",
    "        pmi_0, npmi_0, errors = pmi(topics=topics[0],\n",
    "                                    word_frequency=words_freq,\n",
    "                                    term_docs=words_docs,\n",
    "                                    n_docs=n_docs,\n",
    "                                    n_top_words=top_words)\n",
    "        \n",
    "        if hierarchical:\n",
    "            words_freq, words_docs, n_docs = count_tf_idf_repr(topics[1],\n",
    "                                                                 vocab,\n",
    "                                                                 csr_matrix(cluwords_repr).transpose())\n",
    "            pmi_1, npmi_1, errors = pmi(topics=topics[1],\n",
    "                                    word_frequency=words_freq,\n",
    "                                    term_docs=words_docs,\n",
    "                                    n_docs=n_docs,\n",
    "                                    n_top_words=top_words)\n",
    "            \n",
    "            words_freq, words_docs, n_docs = count_tf_idf_repr(topics[2],\n",
    "                                                                 vocab,\n",
    "                                                                 csr_matrix(cluwords_repr).transpose())\n",
    "            pmi_2, npmi_2, errors = pmi(topics=topics[2],\n",
    "                                    word_frequency=words_freq,\n",
    "                                    term_docs=words_docs,\n",
    "                                    n_docs=n_docs,\n",
    "                                    n_top_words=top_words)\n",
    "            \n",
    "        \n",
    "        total_errors += errors\n",
    "\n",
    "        npmi_0_score.append(np.mean(npmi_0)) \n",
    "        npmi_0_score.append(np.std(npmi_0, ddof=1))\n",
    "        npmi_all += npmi_0\n",
    "        \n",
    "        if hierarchical:\n",
    "            npmi_1_score.append(np.mean(npmi_1)) \n",
    "            npmi_1_score.append(np.std(npmi_1, ddof=1))\n",
    "            npmi_all += npmi_1\n",
    "            \n",
    "            npmi_2_score.append(np.mean(npmi_2)) \n",
    "            npmi_2_score.append(np.std(npmi_2, ddof=1))\n",
    "            npmi_all += npmi_2\n",
    "            \n",
    "            npmi_all_score.append(np.mean(npmi_all)) \n",
    "            npmi_all_score.append(np.std(npmi_all, ddof=1))\n",
    "    \n",
    "#     print(f\"{dataset} {' '.join([str(score) for score in npmi_0_score])} -- {total_errors}\")\n",
    "    if hierarchical:\n",
    "        print(f\"{dataset} {' '.join([str(score) for score in npmi_0_score])} {' '.join([str(score) for score in npmi_1_score])} {' '.join([str(score) for score in npmi_2_score])} {' '.join([str(score) for score in npmi_all_score])}\")\n",
    "    else:\n",
    "        print(f\"{dataset} {' '.join([str(score) for score in npmi_0_score])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

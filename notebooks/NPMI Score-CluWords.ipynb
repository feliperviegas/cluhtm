{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TFIDF Represantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_path):\n",
    "    arq = open(input_path, 'r', encoding=\"utf-8\")\n",
    "    doc = arq.readlines()\n",
    "    arq.close()\n",
    "    documents = list(map(str.rstrip, doc))\n",
    "    n_documents = len(documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    vectorizer = TfidfVectorizer(encoding='utf-8', \n",
    "                                 analyzer='word', \n",
    "                                 max_df=1.0, \n",
    "                                 min_df=1,\n",
    "                                 norm='l2', \n",
    "                                 use_idf=True, \n",
    "                                 smooth_idf=False, \n",
    "                                 sublinear_tf=True)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"wpp\"\n",
    "source_dataset = \"../textual_folds\"\n",
    "dataset_input_file = f\"{source_dataset}/{dataset}Pre.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2956, 1777), 1777)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_input(input_path=dataset_input_file)\n",
    "X, feature_names = tfidf(data=data)\n",
    "X.shape, len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPMI Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tf_idf_repr(topics, cw_words, tf_idf_t):\n",
    "    cw_frequency = {}\n",
    "    cw_docs = {}\n",
    "    for iter_topic in topics:\n",
    "        for word in iter_topic:\n",
    "            word_index = np.where(cw_words == word)[0]\n",
    "            cw_frequency[word] = float(tf_idf_t[word_index].data.shape[0])\n",
    "            cw_docs[word] = set(tf_idf_t[word_index].nonzero()[1])\n",
    "\n",
    "    n_docs = float(tf_idf_t.data.shape[0])\n",
    "\n",
    "    return cw_frequency, cw_docs, n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(topics, word_frequency, term_docs, n_docs, n_top_words):\n",
    "    pmi = []\n",
    "    npmi = []\n",
    "\n",
    "    n_top_words = float(n_top_words)\n",
    "\n",
    "    for t in range(len(topics)):\n",
    "        top_w = topics[t]\n",
    "        # top_w = topico.split(' ')\n",
    "\n",
    "        pmi_t = 0.0\n",
    "        npmi_t = 0.0\n",
    "\n",
    "        for j in range(1, len(top_w)):\n",
    "            for i in range(0, j):\n",
    "                ti = top_w[i]\n",
    "                tj = top_w[j]\n",
    "\n",
    "                c_i = word_frequency[ti]\n",
    "                c_j = word_frequency[tj]\n",
    "                c_i_and_j = len(term_docs[ti].intersection(term_docs[tj]))\n",
    "\n",
    "                pmi_t += np.log(((c_i_and_j + 1.0) / float(n_docs)) /\n",
    "                                ((c_i * c_j) / float(n_docs) ** 2))\n",
    "\n",
    "                npmi_t += -1.0 * np.log((c_i_and_j + 0.01) / float(n_docs))\n",
    "\n",
    "        peso = 1.0 / (n_top_words * (n_top_words - 1.0))\n",
    "\n",
    "        pmi.append(peso * pmi_t)\n",
    "        npmi.append(pmi_t / npmi_t)\n",
    "\n",
    "    return pmi, npmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence(topics, word_frequency, term_docs):\n",
    "    coherence = []\n",
    "\n",
    "    for t in range(len(topics)):\n",
    "        top_w = topics[t]\n",
    "\n",
    "        coherence_t = 0.0\n",
    "        for i in range(1, len(top_w)):\n",
    "            for j in range(0, i):\n",
    "                cont_wi = word_frequency[top_w[j]]\n",
    "                cont_wi_wj = float(\n",
    "                    len(term_docs[top_w[j]].intersection(term_docs[top_w[i]])))\n",
    "                coherence_t += np.log((cont_wi_wj + 1.0) / cont_wi)\n",
    "\n",
    "        coherence.append(coherence_t)\n",
    "\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V-L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def w2v_metric(topics, word_embedding_path, distance_type=\"cos_dist\", top_words=10, embedding_type=False):\n",
    "        word_vectors = KeyedVectors.load_word2vec_format(f\"{word_embedding_path}\", binary=embedding_type)\n",
    "#         model = word_vectors.wv\n",
    "        values = []\n",
    "\n",
    "        for words in topics:\n",
    "#             words = topic.split(' ')\n",
    "            value = calc_dist_2(words, word_vectors, distance_type, top_words)\n",
    "            values.append(value)\n",
    "\n",
    "        return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as sci_dist\n",
    "\n",
    "def calc_dist_2(words, w2v_model, distance_type, t):\n",
    "    l1_dist = 0\n",
    "    l2_dist = 0\n",
    "    cos_dist = 0\n",
    "    coord_dist = 0\n",
    "    t = float(t)\n",
    "\n",
    "    for word_id1 in range(len(words)):\n",
    "        for word_id2 in range(word_id1 + 1, len(words)):\n",
    "            # Calcular L1 w2v metric\n",
    "            l1_dist += (sci_dist.euclidean(\n",
    "                w2v_model[words[word_id1]], w2v_model[words[word_id2]]))\n",
    "\n",
    "            # Calcular L2 w2v metric\n",
    "            l2_dist += (sci_dist.sqeuclidean(\n",
    "                w2v_model[words[word_id1]], w2v_model[words[word_id2]]))\n",
    "\n",
    "            # Calcular cos w2v metric\n",
    "            cos_dist += (sci_dist.cosine(\n",
    "                w2v_model[words[word_id1]], w2v_model[words[word_id2]]))\n",
    "\n",
    "            # Calcular coordinate w2v metric\n",
    "            coord_dist += (sci_dist.sqeuclidean(\n",
    "                w2v_model[words[word_id1]], w2v_model[words[word_id2]]))\n",
    "\n",
    "    if distance_type == 'l1_dist':\n",
    "        return l1_dist / (t * (t - 1.0))\n",
    "    elif distance_type == 'l2_dist':\n",
    "        return l2_dist / (t * (t - 1.0))\n",
    "    elif distance_type == 'cos_dist':\n",
    "        return cos_dist / (t * (t - 1.0))\n",
    "    elif distance_type == 'coord_dist':\n",
    "        return coord_dist / (t * (t - 1.0))\n",
    "\n",
    "    return .0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"wpp\"\n",
    "source = \"../fasttext_wiki_bert_max\"\n",
    "top_words = 5\n",
    "hierarchical_file = f\"{source}/results/{dataset}/hierarchical_struture.txt\"\n",
    "npz_input_file = f\"{source}/results/{dataset}/cluwords_representation_{dataset}.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [], 1: [], 2: []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = {\n",
    "    0: list(),\n",
    "    1: list(),\n",
    "    2: list()\n",
    "}\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hierarchical_file) as hierachical_input:\n",
    "    for topic in hierachical_input:\n",
    "        if topic.startswith(\"\\t\\t\"):\n",
    "            topics[2].append(topic.replace(\"\\t\\t\", \"\").strip().split(\" \"))\n",
    "        elif topic.startswith(\"\\t\"):\n",
    "            topics[1].append(topic.replace(\"\\t\", \"\").strip().split(\" \"))\n",
    "        else:\n",
    "            topics[0].append(topic.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded  = np.load(npz_input_file)\n",
    "cluwords_repr = loaded['tfidf']\n",
    "cluwords_vocab = loaded['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluwords_freq, cluwords_docs, n_docs = count_tf_idf_repr(topics[0],\n",
    "                                                         cluwords_vocab,\n",
    "                                                         csr_matrix(cluwords_repr).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_0, npmi_0 = pmi(\n",
    "                     topics=topics[0],\n",
    "                     word_frequency=cluwords_freq,\n",
    "                     term_docs=cluwords_docs,\n",
    "                     n_docs=n_docs,\n",
    "                     n_top_words=top_words\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_0 = coherence(\n",
    "                      topics=topics[0],\n",
    "                      word_frequency=cluwords_freq,\n",
    "                      term_docs=cluwords_docs\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_score_0 = w2v_metric(\n",
    "                        topics=topics[0], \n",
    "                        word_embedding_path=\"/home/felipeviegas/Codes_phd/cluhtm/wiki-news-300d-1M.vec\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluwords_freq, cluwords_docs, n_docs = count_tf_idf_repr(topics[1],\n",
    "                                                         cluwords_vocab,\n",
    "                                                         csr_matrix(cluwords_repr).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_1, npmi_1 = pmi(topics=topics[1],\n",
    "                word_frequency=cluwords_freq,\n",
    "                term_docs=cluwords_docs,\n",
    "                n_docs=n_docs,\n",
    "                n_top_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_1 = coherence(\n",
    "                      topics=topics[1],\n",
    "                      word_frequency=cluwords_freq,\n",
    "                      term_docs=cluwords_docs\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b38d5ce04e5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m w2v_score_1 = w2v_metric(\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mtopics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mword_embedding_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/felipeviegas/Codes_phd/cluhtm/wiki-news-300d-1M.vec\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       )\n",
      "\u001b[0;32m<ipython-input-9-6b61c23a801f>\u001b[0m in \u001b[0;36mw2v_metric\u001b[0;34m(topics, word_embedding_path, distance_type, top_words, embedding_type)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mw2v_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cos_dist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{word_embedding_path}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#         model = word_vectors.wv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \"\"\"\n\u001b[0;32m-> 1630\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1631\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   1911\u001b[0m             )\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         logger.info(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1818\u001b[0;31m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1819\u001b[0m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_line_to_vector\u001b[0;34m(line, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w2v_score_1 = w2v_metric(\n",
    "                        topics=topics[1], \n",
    "                        word_embedding_path=\"/home/felipeviegas/Codes_phd/cluhtm/wiki-news-300d-1M.vec\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluwords_freq, cluwords_docs, n_docs = count_tf_idf_repr(topics[2],\n",
    "                                                         cluwords_vocab,\n",
    "                                                         csr_matrix(cluwords_repr).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_2, npmi_2 = pmi(topics=topics[2],\n",
    "                word_frequency=cluwords_freq,\n",
    "                term_docs=cluwords_docs,\n",
    "                n_docs=n_docs,\n",
    "                n_top_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_2 = coherence(\n",
    "                      topics=topics[2],\n",
    "                      word_frequency=cluwords_freq,\n",
    "                      term_docs=cluwords_docs\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_score_2 = w2v_metric(\n",
    "                        topics=topics[2], \n",
    "                        word_embedding_path=\"/home/felipeviegas/Codes_phd/cluhtm/wiki-news-300d-1M.vec\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Depth_0 {np.mean(npmi_0)} {np.std(npmi_0, ddof=1)}\")\n",
    "print(f\"Depth_1 {np.mean(npmi_1)} {np.std(npmi_1, ddof=1)}\")\n",
    "print(f\"Depth_2 {np.mean(npmi_2)} {np.std(npmi_2, ddof=1)}\")\n",
    "print(f\"Overall {np.mean(npmi_0 + npmi_1 + npmi_2)} {np.std(npmi_0 + npmi_1 + npmi_2, ddof=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Depth_0 {np.mean(coherence_0)} {np.std(coherence_0, ddof=1)}\")\n",
    "print(f\"Depth_1 {np.mean(coherence_1)} {np.std(coherence_1, ddof=1)}\")\n",
    "print(f\"Depth_2 {np.mean(coherence_2)} {np.std(coherence_2, ddof=1)}\")\n",
    "print(f\"Overall {np.mean(coherence_0 + coherence_1 + coherence_2)} {np.std(coherence_0 + coherence_1 + coherence_2, ddof=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Depth_0 {np.mean(w2v_score_0)} {np.std(w2v_score_0, ddof=1)}\")\n",
    "print(f\"Depth_1 {np.mean(w2v_score_1)} {np.std(w2v_score_1, ddof=1)}\")\n",
    "print(f\"Depth_2 {np.mean(w2v_score_2)} {np.std(w2v_score_2, ddof=1)}\")\n",
    "print(f\"Overall {np.mean(w2v_score_0 + w2v_score_1 + w2v_score_2)} {np.std(w2v_score_0 + w2v_score_1 + w2v_score_2, ddof=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing through TFIDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_freq, features_docs, n_docs = count_tf_idf_repr(topics[0],\n",
    "                                                         np.asarray(feature_names),\n",
    "                                                         csr_matrix(X).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_0, npmi_0 = pmi(topics=topics[0],\n",
    "                    word_frequency=features_freq,\n",
    "                    term_docs=features_docs,\n",
    "                    n_docs=n_docs,\n",
    "                    n_top_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Depth_0 {np.mean(npmi_0)} {np.std(npmi_0, ddof=1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

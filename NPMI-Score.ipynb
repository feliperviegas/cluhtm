{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook computes the NPMI score. We added this notebook in the root path because it computes the CluWords representation while the other notebook located in the notebook's path only read the log files to create the CluWords representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TFIDF Represantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_path):\n",
    "    arq = open(input_path, 'r', encoding=\"utf-8\")\n",
    "    doc = arq.readlines()\n",
    "    arq.close()\n",
    "    documents = list(map(str.rstrip, doc))\n",
    "    n_documents = len(documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    vectorizer = TfidfVectorizer(encoding='utf-8', \n",
    "                                 analyzer='word', \n",
    "                                 max_df=1.0, \n",
    "                                 min_df=1,\n",
    "                                 norm='l2', \n",
    "                                 use_idf=True, \n",
    "                                 smooth_idf=False, \n",
    "                                 sublinear_tf=True)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfidf(dataset_input_file):\n",
    "    data = read_input(input_path=dataset_input_file)\n",
    "    X, feature_names = tfidf(data=data)\n",
    "    \n",
    "    return X, np.asarray(feature_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPMI Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tf_idf_repr(topics, cw_words, tf_idf_t):\n",
    "    cw_frequency = {}\n",
    "    cw_docs = {}\n",
    "    for iter_topic in topics:\n",
    "        for word in iter_topic:\n",
    "            word_index = np.where(cw_words == word)[0]\n",
    "            cw_frequency[word] = float(tf_idf_t[word_index].data.shape[0])\n",
    "            cw_docs[word] = set(tf_idf_t[word_index].nonzero()[1])\n",
    "\n",
    "    n_docs = float(tf_idf_t.data.shape[0])\n",
    "\n",
    "    return cw_frequency, cw_docs, n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(topics, word_frequency, term_docs, n_docs, n_top_words):\n",
    "    pmi = []\n",
    "    npmi = []\n",
    "    n_exceptions = 0\n",
    "\n",
    "    n_top_words = float(n_top_words)\n",
    "\n",
    "    for t in range(len(topics)):\n",
    "        top_w = topics[t]\n",
    "        # top_w = topico.split(' ')\n",
    "\n",
    "        pmi_t = 0.0\n",
    "        npmi_t = 0.0\n",
    "\n",
    "        for j in range(1, len(top_w)):\n",
    "            for i in range(0, j):\n",
    "                ti = top_w[i]\n",
    "                tj = top_w[j]\n",
    "\n",
    "                c_i = word_frequency[ti]\n",
    "                c_j = word_frequency[tj]\n",
    "                c_i_and_j = len(term_docs[ti].intersection(term_docs[tj]))\n",
    "                \n",
    "                try:\n",
    "                    pmi_t += np.log(((c_i_and_j + 1.0) / float(n_docs)) /\n",
    "                                    ((c_i * c_j) / float(n_docs) ** 2))\n",
    "                except ZeroDivisionError:\n",
    "                    n_exceptions += 1\n",
    "                    pmi_t += .0\n",
    "\n",
    "                npmi_t += -1.0 * np.log((c_i_and_j + 0.01) / float(n_docs))\n",
    "\n",
    "        peso = 1.0 / (n_top_words * (n_top_words - 1.0))\n",
    "\n",
    "        pmi.append(peso * pmi_t)\n",
    "        npmi.append(pmi_t / npmi_t)\n",
    "\n",
    "    return pmi, npmi, n_exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hierarchical(option):\n",
    "    topics = {\n",
    "        0: list(),\n",
    "        1: list(),\n",
    "        2: list()\n",
    "    }\n",
    "    if option == 'hpam':\n",
    "        with open(hierarchical_file) as hierachical_input:\n",
    "            for topic in hierachical_input:\n",
    "                if topic.startswith(\"Super-topic\"):\n",
    "                    topics[1].append(topic.replace(\"\\t\", \" \").strip().split(\" \")[4:])\n",
    "                elif topic.startswith(\"Root:\"):\n",
    "                    topics[0].append(topic.replace(\"]\", \" \").strip().split(\" \")[2:])\n",
    "                elif re.match(\"[0-9]+:\", topic):\n",
    "                    topics[2].append(topic.replace(\"\\t\", \" \").strip().split(\" \")[2:])\n",
    "\n",
    "            hierachical_input.close()\n",
    "\n",
    "    else:\n",
    "        with open(hierarchical_file) as hierachical_input:\n",
    "            for topic in hierachical_input:\n",
    "                if topic.startswith(\"\\t\\t\"):\n",
    "                    topics[2].append(topic.replace(\"\\t\\t\", \"\").strip().split(\" \"))\n",
    "                elif topic.startswith(\"\\t\"):\n",
    "                    topics[1].append(topic.replace(\"\\t\", \"\").strip().split(\" \"))\n",
    "                else:\n",
    "                    topics[0].append(topic.strip().split(\" \"))\n",
    "\n",
    "            hierachical_input.close()\n",
    "        \n",
    "    \n",
    "    return topics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['wpp', 'trip', 'acm']\n",
    "# datasets = ['wpp','ang','drop','ever','face','info','pinter','trip','tweets','uber','acm','20News']\n",
    "# method = 'hpam'\n",
    "method = 'cw'\n",
    "\n",
    "# 'tfidf' or 'cw'\n",
    "base_npmi_score = 'cw'\n",
    "hierarchical = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npz(npz_input_file):\n",
    "    loaded  = np.load(npz_input_file)\n",
    "    cluwords_repr = loaded['tfidf']\n",
    "    cluwords_vocab = loaded['feature_names']\n",
    "    \n",
    "    return cluwords_repr, cluwords_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from cluwords import Cluwords, CluwordsTFIDF\n",
    "\n",
    "def gen_cluwords(word_count: int, embedding_file_path: str, dataset: str, datasets_path: str) -> Tuple[np.array, np.array]:\n",
    "    Cluwords(algorithm=\"knn_cosine\",\n",
    "                embedding_file_path=embedding_file_path,\n",
    "                n_words=word_count,\n",
    "                k_neighbors=500,\n",
    "                threshold=0.4,\n",
    "                n_jobs=4,\n",
    "                dataset=dataset\n",
    "    )\n",
    "\n",
    "    cluwords = CluwordsTFIDF(\n",
    "        dataset=dataset,\n",
    "        dataset_file_path=datasets_path,\n",
    "        n_words=word_count,\n",
    "        path_to_save_cluwords=\".\",\n",
    "        class_file_path=\".\"\n",
    "    )\n",
    "\n",
    "    return cluwords.fit_transform(), cluwords.vocab_cluwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN...\n",
      "N Threads: 4\n",
      "NearestNeighbors K=500\n",
      "Time 0.0023139869999795337\n",
      "NN Distaces\n",
      "Time 0.4293993559995215\n",
      "Saving cluwords\n",
      "Matrix(1523, 1523)\n",
      "Number of cluwords 1523\n",
      "Matrix(1523, 1523)\n",
      "\n",
      "Computing TF...\n",
      "tf shape (2956, 1523)\n",
      "Cluwords TF done in 0.080s.\n",
      "\n",
      "Computing IDF...\n",
      "Read data\n",
      "Time 0.033854158999929496\n",
      "Dot tf and hyp_aux\n",
      "Time 0.07118862900006206\n",
      "Divide hyp_aux by itself\n",
      "Time 0.06565197200052353\n",
      "Dot tf and bin hyp_aux\n",
      "Time 0.10804990900032863\n",
      "Divide _dot and _dot_bin\n",
      "Time 0.04342712200013921\n",
      "Sum\n",
      "Time 0.0016727610000089044\n",
      "log\n",
      "Time 6.276700059970608e-05\n",
      "kNN...\n",
      "N Threads: 4\n",
      "NearestNeighbors K=500\n",
      "Time 0.0037983800002621138\n",
      "NN Distaces\n",
      "Time 0.7238689619998695\n",
      "Saving cluwords\n",
      "Matrix(2622, 2622)\n",
      "Number of cluwords 2622\n",
      "Matrix(2622, 2622)\n",
      "\n",
      "Computing TF...\n",
      "tf shape (2816, 2622)\n",
      "Cluwords TF done in 0.230s.\n",
      "\n",
      "Computing IDF...\n",
      "Read data\n",
      "Time 0.06438148100005492\n",
      "Dot tf and hyp_aux\n",
      "Time 0.13713144599933003\n",
      "Divide hyp_aux by itself\n",
      "Time 0.08956939600011538\n",
      "Dot tf and bin hyp_aux\n",
      "Time 0.176406665000286\n",
      "Divide _dot and _dot_bin\n",
      "Time 0.09978450299968245\n",
      "Sum\n",
      "Time 0.003103316000306222\n",
      "log\n",
      "Time 0.00010385900077380938\n",
      "kNN...\n",
      "N Threads: 4\n",
      "NearestNeighbors K=500\n",
      "Time 0.01966971299953002\n",
      "NN Distaces\n",
      "Time 10.123806714999773\n",
      "Saving cluwords\n",
      "Matrix(8465, 8465)\n",
      "Number of cluwords 8465\n",
      "Matrix(8465, 8465)\n",
      "\n",
      "Computing TF...\n",
      "tf shape (24888, 8465)\n",
      "Cluwords TF done in 20.408s.\n",
      "\n",
      "Computing IDF...\n",
      "Read data\n",
      "Time 1.1725214890002462\n",
      "Dot tf and hyp_aux\n",
      "Time 16.735430982000253\n",
      "Divide hyp_aux by itself\n",
      "Time 0.5500063210001827\n",
      "Dot tf and bin hyp_aux\n",
      "Time 16.88565636399926\n",
      "Divide _dot and _dot_bin\n",
      "Time 1.8038678700004311\n",
      "Sum\n",
      "Time 0.061357309999948484\n",
      "log\n",
      "Time 0.0001167880000139121\n"
     ]
    }
   ],
   "source": [
    "result_scores = []\n",
    "for dataset in datasets:\n",
    "    total_errors = 0\n",
    "    npmi_0_score = list()\n",
    "    npmi_1_score = list()\n",
    "    npmi_2_score = list()\n",
    "    npmi_all_score = list()\n",
    "#     top_sets = [5, 10, 20]\n",
    "    top_sets = [10]\n",
    "    \n",
    "    if base_npmi_score == 'cw':\n",
    "        # cw_source = '../fasttext_wiki_bert_max'\n",
    "        cw_source = \"fasttext_wiki\"\n",
    "        source_dataset = \"textual_folds\"\n",
    "        npz_input_file = f\"{cw_source}/results/{dataset}_seed-42/cluwords_representation_{dataset}.npz\"\n",
    "        # npz_input_file = f\"{cw_source}/results/{dataset}/cluwords_representation_{dataset}.npz\"\n",
    "        # cluwords_repr, vocab = read_npz(npz_input_file)\n",
    "        embeddings_path = f\"{cw_source}/datasets/gn_w2v_models/{dataset}.txt\"\n",
    "        word_count = int(open(embeddings_path, \"r\").readline().strip().split(\" \")[0])\n",
    "        cluwords_repr, vocab = gen_cluwords(word_count=word_count, \n",
    "                                            embedding_file_path=embeddings_path, \n",
    "                                            dataset=dataset, \n",
    "                                            datasets_path=f\"{source_dataset}/{dataset}Pre.txt\")\n",
    "        \n",
    "    else: # 'tfidf'\n",
    "        source_dataset = \"textual_folds\"\n",
    "        dataset_input_file = f\"{source_dataset}/{dataset}Pre.txt\"\n",
    "        cluwords_repr, vocab = read_tfidf(dataset_input_file)\n",
    "\n",
    "    for top_words in top_sets:\n",
    "        npmi_all = list()\n",
    "        if method == 'cw':\n",
    "            source = \"fasttext_wiki\"\n",
    "            # source = \"../fasttext_wiki_bert_max\"\n",
    "            # hierarchical_file = f\"{source}/results/{dataset}/hierarchical_struture.txt\"\n",
    "            hierarchical_file = f\"{source}/results/{dataset}_seed-42/hierarchical_struture.txt\"\n",
    "        elif method == 'bertopic':\n",
    "            source = \"../BertTopicResults\"\n",
    "            hierarchical_file = f\"{source}/topic_words_{top_words}_{dataset}Pre\"\n",
    "        elif method == 'hpam':\n",
    "            source = \"../HPAMResults\"\n",
    "            hierarchical_file = f\"{source}/{dataset}.txt\"\n",
    "                \n",
    "\n",
    "        topics = read_hierarchical(method)\n",
    "\n",
    "        words_freq, words_docs, n_docs = count_tf_idf_repr(topics[0],\n",
    "                                                                 vocab,\n",
    "                                                                 csr_matrix(cluwords_repr).transpose())\n",
    "\n",
    "        pmi_0, npmi_0, errors = pmi(topics=topics[0],\n",
    "                                    word_frequency=words_freq,\n",
    "                                    term_docs=words_docs,\n",
    "                                    n_docs=n_docs,\n",
    "                                    n_top_words=top_words)\n",
    "        \n",
    "        if hierarchical:\n",
    "            words_freq, words_docs, n_docs = count_tf_idf_repr(topics[1],\n",
    "                                                                 vocab,\n",
    "                                                                 csr_matrix(cluwords_repr).transpose())\n",
    "            pmi_1, npmi_1, errors = pmi(topics=topics[1],\n",
    "                                    word_frequency=words_freq,\n",
    "                                    term_docs=words_docs,\n",
    "                                    n_docs=n_docs,\n",
    "                                    n_top_words=top_words)\n",
    "            \n",
    "            words_freq, words_docs, n_docs = count_tf_idf_repr(topics[2],\n",
    "                                                                 vocab,\n",
    "                                                                 csr_matrix(cluwords_repr).transpose())\n",
    "            pmi_2, npmi_2, errors = pmi(topics=topics[2],\n",
    "                                    word_frequency=words_freq,\n",
    "                                    term_docs=words_docs,\n",
    "                                    n_docs=n_docs,\n",
    "                                    n_top_words=top_words)\n",
    "            \n",
    "        \n",
    "        total_errors += errors\n",
    "\n",
    "        npmi_0_score.append(np.mean(npmi_0)) \n",
    "        npmi_0_score.append(np.std(npmi_0, ddof=1))\n",
    "        npmi_0_score.append(len(npmi_0))\n",
    "        npmi_all += npmi_0\n",
    "        \n",
    "        if hierarchical:\n",
    "            npmi_1_score.append(np.mean(npmi_1)) \n",
    "            npmi_1_score.append(np.std(npmi_1, ddof=1))\n",
    "            npmi_1_score.append(len(npmi_1))\n",
    "            npmi_all += npmi_1\n",
    "            \n",
    "            npmi_2_score.append(np.mean(npmi_2)) \n",
    "            npmi_2_score.append(np.std(npmi_2, ddof=1))\n",
    "            npmi_2_score.append(len(npmi_2))\n",
    "            npmi_all += npmi_2\n",
    "            \n",
    "            npmi_all_score.append(np.mean(npmi_all)) \n",
    "            npmi_all_score.append(np.std(npmi_all, ddof=1))\n",
    "            npmi_all_score.append(len(npmi_all_score))\n",
    "    \n",
    "#     print(f\"{dataset} {' '.join([str(score) for score in npmi_0_score])} -- {total_errors}\")\n",
    "    if hierarchical:\n",
    "        result_scores.append(f\"{dataset} {' '.join([str(score) for score in npmi_0_score])} {' '.join([str(score) for score in npmi_1_score])} {' '.join([str(score) for score in npmi_2_score])} {' '.join([str(score) for score in npmi_all_score])}\")\n",
    "    else:\n",
    "        result_scores.append(f\"{dataset} {' '.join([str(score) for score in npmi_0_score])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wpp 0.9729594510966713 0.007028090616159849 5 0.9624725859577038 0.011261918607699081 59 0.9559611529171758 0.0202587039156831 238 0.9575146828519066 0.018945586738232042 2\n",
      "trip 0.976717514170312 0.006624623386081015 7 0.9689831890435137 0.0162268053664875 65 0.9665342321216167 0.016753526697464563 280 0.9671889627303216 0.016571509466998504 2\n",
      "acm 0.9673157227883215 0.015431302744087246 7 0.9597710458080372 0.01475043862920575 38 0.9555658916263627 0.014147945156229583 268 0.9563391973037982 0.014364098469128523 2\n"
     ]
    }
   ],
   "source": [
    "for result in result_scores:\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

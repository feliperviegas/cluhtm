{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to study the CluWords representation. There are some plots to understand the co-occurrence among the features created by the CluWords representation in contrast to the TFIDF representation. \n",
    "This notebook is located in the root path because it run the CluWords representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TFIDF Represantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_path):\n",
    "    arq = open(input_path, 'r', encoding=\"utf-8\")\n",
    "    doc = arq.readlines()\n",
    "    arq.close()\n",
    "    documents = list(map(str.rstrip, doc))\n",
    "    n_documents = len(documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    vectorizer = TfidfVectorizer(encoding='utf-8', \n",
    "                                 analyzer='word', \n",
    "                                 max_df=1.0, \n",
    "                                 min_df=1,\n",
    "                                 norm='l2', \n",
    "                                 use_idf=True, \n",
    "                                 smooth_idf=False, \n",
    "                                 sublinear_tf=True)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfidf(dataset_input_file):\n",
    "    data = read_input(input_path=dataset_input_file)\n",
    "    X, feature_names = tfidf(data=data)\n",
    "    \n",
    "    return X, np.asarray(feature_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['wpp']\n",
    "# datasets = ['wpp','ang','drop','ever','face','info','pinter','trip','tweets','uber','acm','20News']\n",
    "# method = 'hpam'\n",
    "method = 'cw'\n",
    "\n",
    "# 'tfidf' or 'cw'\n",
    "base_npmi_score = 'cw'\n",
    "hierarchical = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npz(npz_input_file):\n",
    "    loaded  = np.load(npz_input_file)\n",
    "    cluwords_repr = loaded['tfidf']\n",
    "    cluwords_vocab = loaded['feature_names']\n",
    "    \n",
    "    return cluwords_repr, cluwords_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from cluwords import Cluwords, CluwordsTFIDF\n",
    "\n",
    "def gen_cluwords(word_count: int, embedding_file_path: str, dataset: str, datasets_path: str) -> Tuple[np.array, np.array]:\n",
    "    Cluwords(algorithm=\"knn_cosine\",\n",
    "                embedding_file_path=embedding_file_path,\n",
    "                n_words=word_count,\n",
    "                k_neighbors=500,\n",
    "                threshold=0.4,\n",
    "                n_jobs=4,\n",
    "                dataset=dataset\n",
    "    )\n",
    "\n",
    "    cluwords = CluwordsTFIDF(\n",
    "        dataset=dataset,\n",
    "        dataset_file_path=datasets_path,\n",
    "        n_words=word_count,\n",
    "        path_to_save_cluwords=\".\",\n",
    "        class_file_path=\".\"\n",
    "    )\n",
    "\n",
    "    return cluwords.fit_transform(), cluwords.vocab_cluwords\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab\n",
    "import networkx as nx\n",
    "\n",
    "def save_graph(graph,file_name):\n",
    "    #initialze Figure\n",
    "    plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    pos = nx.spring_layout(graph, k=0.5, iterations=20)\n",
    "    colors = [node[1]['color'] for node in graph.nodes(data=True)]\n",
    "    nx.draw_networkx_nodes(graph,pos, node_color=colors, node_size=200)\n",
    "    nx.draw_networkx_edges(graph,pos)\n",
    "    # nx.draw_networkx_labels(graph,pos)\n",
    "\n",
    "    plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "    pylab.close()\n",
    "    plt.close()\n",
    "    del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def barplot(filename, df, y):\n",
    "    plt.figure(figsize=(20, 20), dpi=80)\n",
    "    sns.barplot(data=df, x=\"doc_id\", y=y)\n",
    "    # plt.ylabel('Value')\n",
    "    # plt.xlabel('Index')\n",
    "    # plt.grid()\n",
    "    # # plt.legend(handles=legend_handles, bbox_to_anchor=(1.02, 1), title='Column')\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(filename,bbox_inches=\"tight\")\n",
    "    pylab.close()\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tfidf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[0]\n",
    "source_dataset = \"textual_folds\"\n",
    "dataset_input_file = f\"{source_dataset}/{dataset}Pre.txt\"\n",
    "tfidf_repr, vocab_tf_idf = read_tfidf(dataset_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['application', 'update', 'messenger', ..., 'withdraw', 'setup',\n",
       "       'manually'], dtype='<U15')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = 5\n",
    "max_indexes = np.argsort(np.asarray(tfidf_repr.sum(axis=0)).flatten())[::-1]\n",
    "np.asarray(vocab_tf_idf)[max_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occ_tfidf = tfidf_repr.transpose().dot(tfidf_repr)\n",
    "bin_co_occ_tfidf = (co_occ_tfidf>0)*1\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for x in max_indexes[:top_words]:\n",
    "    G.add_node(vocab_tf_idf[x], color='orange')\n",
    "    for y in range(bin_co_occ_tfidf.shape[1]):\n",
    "        if bin_co_occ_tfidf[x, y] and x != y :\n",
    "            if not vocab_tf_idf[y] in vocab_tf_idf[max_indexes[:top_words]]:\n",
    "                G.add_node(vocab_tf_idf[y], color='blue')\n",
    "                \n",
    "            G.add_edge(vocab_tf_idf[x], vocab_tf_idf[y], weight=4)\n",
    "\n",
    "save_graph(G, \"graph_tfidf.pdf\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CluWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN...\n",
      "N Threads: 4\n",
      "NearestNeighbors K=500\n",
      "Time 0.005459096000038244\n",
      "NN Distaces\n",
      "Time 0.3758323150000251\n",
      "Saving cluwords\n",
      "Matrix(1523, 1523)\n",
      "Number of cluwords 1523\n",
      "Matrix(1523, 1523)\n",
      "\n",
      "Computing TF...\n",
      "tf shape (2956, 1523)\n",
      "Cluwords TF done in 0.082s.\n",
      "\n",
      "Computing IDF...\n",
      "Read data\n",
      "Time 0.03401001299999962\n",
      "Dot tf and hyp_aux\n",
      "Time 0.045774392000055286\n",
      "Divide hyp_aux by itself\n",
      "Time 0.026231893999977274\n",
      "Dot tf and bin hyp_aux\n",
      "Time 0.04536375100002488\n",
      "Divide _dot and _dot_bin\n",
      "Time 0.04238399799999115\n",
      "Sum\n",
      "Time 0.001355369000009432\n",
      "log\n",
      "Time 5.123400001139089e-05\n"
     ]
    }
   ],
   "source": [
    "cw_source = \"fasttext_wiki-bert_concat-using-distinct-seeds\"\n",
    "source_dataset = \"textual_folds\"\n",
    "npz_input_file = f\"{cw_source}/results/{dataset}_seed-42/cluwords_representation_{dataset}.npz\"\n",
    "# npz_input_file = f\"{cw_source}/results/{dataset}/cluwords_representation_{dataset}.npz\"\n",
    "# cluwords_repr, vocab = read_npz(npz_input_file)\n",
    "embeddings_path = f\"{cw_source}/datasets/gn_w2v_models/{dataset}.txt\"\n",
    "word_count = int(open(embeddings_path, \"r\").readline().strip().split(\" \")[0])\n",
    "cluwords_repr, vocab_cw = gen_cluwords(word_count=word_count, \n",
    "                                    embedding_file_path=embeddings_path, \n",
    "                                    dataset=dataset, \n",
    "                                    datasets_path=f\"{source_dataset}/{dataset}Pre.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['application', 'chat', 'download', 'messenger', 'update'],\n",
       "       dtype='<U14'),\n",
       " array(['application', 'update', 'messenger', 'chat', 'download'],\n",
       "       dtype='<U15'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_indexes_cw = []\n",
    "for idx in range(vocab_cw.shape[0]):\n",
    "    if vocab_cw[idx] in vocab_tf_idf[max_indexes[:top_words]]:\n",
    "        max_indexes_cw.append(idx)\n",
    "\n",
    "max_indexes_cw = np.asarray(max_indexes_cw)\n",
    "\n",
    "vocab_cw[max_indexes_cw], vocab_tf_idf[max_indexes[:top_words]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occ_cluwords = np.dot(cluwords_repr.T, cluwords_repr)\n",
    "bin_co_occu_cluwords = (co_occ_cluwords>0)*1\n",
    "\n",
    "G_cw = nx.Graph()\n",
    "\n",
    "for x in max_indexes_cw: \n",
    "    G_cw.add_node(vocab_cw[x], color='orange')\n",
    "    for y in range(bin_co_occu_cluwords.shape[1]):\n",
    "        if bin_co_occu_cluwords[x, y] and x != y:\n",
    "            if not vocab_cw[y] in vocab_cw[max_indexes_cw]:\n",
    "                G_cw.add_node(vocab_cw[y], color='blue')\n",
    "                \n",
    "            G_cw.add_edge(vocab_cw[x], vocab_cw[y], weight=4)\n",
    "\n",
    "save_graph(G_cw, \"graph_cw.pdf\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "bin_cluwords_repr = (cluwords_repr > 0) * 1\n",
    "bin_tfidf_repr = (tfidf_repr > 0) * 1\n",
    "sum_tokens_cw = bin_cluwords_repr.sum(axis=1)\n",
    "sum_tokens_tfidf = np.asarray(bin_tfidf_repr.sum(axis=1)).flatten()\n",
    "\n",
    "pd_cw = pd.DataFrame({\"doc_id\": np.arange(sum_tokens_cw.shape[0]), \"cw_count\": sum_tokens_cw, \"tfidf_count\": sum_tokens_tfidf})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"bar_plot_tfidf.pdf\", pd_cw, \"tfidf_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"bar_plot_cw.pdf\", pd_cw, \"cw_count\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if Norm impacts the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "file = open(\"/home/local/FARFETCH/felipe.viegas/repo/cluhtm/fasttext_wiki-bert_concat-using-distinct-seeds/datasets/gn_w2v_models/wpp.txt\", \"r\")\n",
    "file.readline()\n",
    "\n",
    "embeddings = []\n",
    "for line in file:\n",
    "    features = line.strip().split(\" \")[1:]\n",
    "    embedding = [dim for dim in features]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "embeddings = np.asarray(embeddings, dtype=np.float32)\n",
    "\n",
    "#Normalization\n",
    "norm_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# Pairwise cosine similarity\n",
    "cos_sim = cosine_similarity(embeddings)\n",
    "cos_sim_norm = cosine_similarity(norm_embeddings)\n",
    "\n",
    "for emb, n_emb in zip(cos_sim.round(decimals=1), cos_sim_norm.round(decimals=1)):\n",
    "    if not np.array_equal(emb, n_emb):\n",
    "        print(\"----------------------------------\")\n",
    "        print(emb)\n",
    "        print(n_emb)\n",
    "        print(\"----------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization does not affect the cosine similarity distribution, but it will affect its precision. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluwords_emb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd5b5d06706bb19d070e38d4ebad3eab8249ed08b8bc628803b1920b913342d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
